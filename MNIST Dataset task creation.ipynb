{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, SimpleRNN\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)\n",
    "\n",
    "# resize and normalize\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train,[-1, image_size * image_size])\n",
    "x_test = np.reshape(x_test,[-1, image_size* image_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 size: Trainset - (12665, 784), (12665, 10), Testset - (2115, 784), (2115, 10)\n",
      "Task 1 size: Trainset - (11263, 784), (11263, 10), Testset - (1874, 784), (1874, 10)\n",
      "Task 2 size: Trainset - (12183, 784), (12183, 10), Testset - (1986, 784), (1986, 10)\n",
      "Task 3 size: Trainset - (11800, 784), (11800, 10), Testset - (1983, 784), (1983, 10)\n",
      "Task 4 size: Trainset - (12089, 784), (12089, 10), Testset - (2042, 784), (2042, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "np.random.seed(100)\n",
    "n_tasks = 5\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#task_labels = [[0,1], [2,3], [4,5], [6,7], [8,9]]\n",
    "task_labels = [[0,1], [4,5], [6,7], [8,9], [2,3]]\n",
    "#task_labels = [[4,2], [0,6], [3,8], [9,7], [1,5],[8,9],[6,7],[5,5],[3,2],[0,1]]\n",
    "#task_labels = [[16,14], [25, 7], [20, 27], [15, 1], [32, 19]]\n",
    "#task_labels = [[0,9], [7,8], [3,6], [1,4], [2,5]]\n",
    "#task_labels = [[9, 3], [1, 8], [7, 4], [0, 5], [6, 2]]\n",
    "#task_labels = [[0,1], [2,3,1,0],[4,5,1,2], [6,7,3,0],[8,9,4,6]]\n",
    "n_tasks = len(task_labels)\n",
    "nb_classes  = 10\n",
    "training_datasets = []\n",
    "validation_datasets = []\n",
    "multihead=False\n",
    "\n",
    "for labels in task_labels:\n",
    "    idx = np.in1d(y_train, labels)\n",
    "    if multihead:\n",
    "        label_map = np.arange(nb_classes)\n",
    "        label_map[labels] = np.arange(len(labels))\n",
    "        data = x_train[idx], np_utils.to_categorical(label_map[y_train[idx]], len(labels))\n",
    "    else:\n",
    "        data = x_train[idx], np_utils.to_categorical(y_train[idx], nb_classes)\n",
    "        training_datasets.append(data)\n",
    "\n",
    "for labels in task_labels:\n",
    "    idx = np.in1d(y_test, labels)\n",
    "    if multihead:\n",
    "        label_map = np.arange(nb_classes)\n",
    "        label_map[labels] = np.arange(len(labels))\n",
    "        data = x_test[idx], np_utils.to_categorical(label_map[y_test[idx]], len(labels))\n",
    "    else:\n",
    "        data = x_test[idx], np_utils.to_categorical(y_test[idx], nb_classes)\n",
    "        validation_datasets.append(data)\n",
    "        \n",
    "tasks_train={}; labels_train = {}; tasks_test = {}; labels_test = {}\n",
    "\n",
    "for i in range(len(task_labels)):\n",
    "    tasks_train[str(i)] = training_datasets[i][0]\n",
    "    labels_train[str(i)] = training_datasets[i][1]\n",
    "    tasks_test[str(i)] = validation_datasets[i][0]\n",
    "    labels_test[str(i)] = validation_datasets[i][1]\n",
    "    print('Task {0} size: Trainset - {1}, {2}, Testset - {3}, {4}'.format(i,tasks_train[str(i)].shape, labels_train[str(i)].shape, tasks_test[str(i)].shape, labels_test[str(i)].shape))\n",
    "\n",
    "Tasks_dumped = []\n",
    "for i in range(len(task_labels)):\n",
    "    Tasks_dumped.append((tasks_train[str(i)], labels_train[str(i)], tasks_test[str(i)], labels_test[str(i)], tasks_test[str(i)], labels_test[str(i)]))\n",
    "f = open('mnist_tasks.pkl', \"wb\")\n",
    "pickle.dump(Tasks_dumped, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
