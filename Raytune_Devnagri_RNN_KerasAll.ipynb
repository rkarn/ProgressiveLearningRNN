{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from ray.tune import track\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "try:\n",
    "    tf.get_logger().setLevel('INFO')\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import ray\n",
    "from ray import tune\n",
    "import time\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "class TuneReporterCallback(keras.callbacks.Callback):\n",
    "    \"\"\"Tune Callback for Keras.\n",
    "    \n",
    "    The callback is invoked every epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logs={}):\n",
    "        self.iteration = 0\n",
    "        super(TuneReporterCallback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.iteration += 1\n",
    "        track.log(keras_info=logs, mean_accuracy=logs.get(\"accuracy\"), mean_loss=logs.get(\"loss\"))\n",
    "    \n",
    "'''   \n",
    "class TuneReporterCallback(keras.callbacks.Callback):\n",
    "    \"\"\"Tune Callback for Keras.\n",
    "    \n",
    "    The callback is invoked every epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, logs={}):\n",
    "        self.iteration = 0\n",
    "        super(TuneReporterCallback, self).__init__()\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.iteration += 1\n",
    "        if \"acc\" in logs:\n",
    "            tune.report(keras_info=logs, val_loss=logs['val_loss'], mean_accuracy=logs[\"acc\"], f1_m = logs['f1_m'], precision_m=logs['precision_m'], recall_m=logs['recall_m'])\n",
    "        else:\n",
    "            tune.report(keras_info=logs, val_loss=logs['val_loss'], mean_accuracy=logs.get(\"accuracy\"), f1_m = logs.get('f1_m'), precision_m=logs.get('precision_m'), recall_m=logs.get('recall_m'))\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "from keras import backend as K\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))        \n",
    "        \n",
    "def create_model(learning_rate, RNN_units, dropout):\n",
    "    assert learning_rate > 0 and RNN_units > 0 and dropout > 0, \"Did you set the right configuration?\"\n",
    "    input_shape = (32, 32)\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=int(RNN_units), dropout=dropout, input_shape=input_shape,  activation='relu', name='RNN'))\n",
    "    model.add(Dense(num_labels, activation = 'softmax', name = 'dense_output'))\n",
    "    #optimizer = SGD(lr=learning_rate)\n",
    "    optimizer = Adam(clipvalue=0.5, lr=learning_rate)\n",
    "    model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m])\n",
    "    return model\n",
    "        \n",
    "def tune_devnagri(config):  \n",
    "    model = create_model(learning_rate=config['lr'], RNN_units=int(config['unit']), dropout=config['dropout'])  # TODO: Change me.\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        \"model.h5\", monitor='loss', save_best_only=True, save_freq=2)\n",
    "\n",
    "    # Enable Tune to make intermediate decisions by using a Tune Callback hook. This is Keras specific.\n",
    "    callbacks = [checkpoint_callback, TuneReporterCallback()]\n",
    "    task_dataset = pickle.load(open('/root/Raytune_Devnagri_RNN/task_dataset.pkl', \"rb\"))\n",
    "    image_size = 32\n",
    "    X_train = np.reshape(task_dataset[0],[-1, image_size, image_size]) \n",
    "    Y_train = task_dataset[1]\n",
    "    X_test = np.reshape(task_dataset[2],[-1, image_size, image_size]) \n",
    "    Y_test = task_dataset[3]\n",
    "    # Train the model\n",
    "    hist = model.fit(\n",
    "        X_train, Y_train, \n",
    "        validation_data=(X_test, Y_test),\n",
    "        verbose=0, \n",
    "        batch_size=100, \n",
    "        epochs=10, \n",
    "        callbacks=callbacks)\n",
    "    for key in hist.history:\n",
    "        print(key)\n",
    "\n",
    "# Random and uniform sampling for hypertune\n",
    "def random_search(task_data, task_id=0, cpu_resouce_fed=4):\n",
    "    import numpy as np; np.random.seed(5)  \n",
    "    hyperparameter_space = {\n",
    "        \"lr\": tune.loguniform(0.001, 0.1),  \n",
    "        \"unit\": tune.uniform(50, 80),\n",
    "        \"dropout\": tune.loguniform(0.01, 0.3),\n",
    "    }  \n",
    "    num_samples = 10 \n",
    "    ####################################################################################################\n",
    "    ################ This is just a validation function for tutorial purposes only. ####################\n",
    "    HP_KEYS = [\"lr\", \"unit\", \"dropout\"]\n",
    "    assert all(key in hyperparameter_space for key in HP_KEYS), (\n",
    "        \"The hyperparameter space is not fully designated. It must include all of {}\".format(HP_KEYS))\n",
    "    ######################################################################################################\n",
    "\n",
    "    ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "    ray.init(log_to_driver=False, num_cpus=cpu_resouce_fed)\n",
    "    # We clean out the logs before running for a clean visualization later.\n",
    "    ! rm -rf ~/ray_results/tune_devnagri$task_id\n",
    "    analysis = tune.run(\n",
    "        tune_devnagri, \n",
    "        name=\"Random_devnagri_task\"+str(task_id),\n",
    "        verbose=1, \n",
    "        config=hyperparameter_space,\n",
    "        num_samples=num_samples)\n",
    "    time.sleep(1)\n",
    "\n",
    "    assert len(analysis.trials) > 2, \"Did you set the correct number of samples?\"\n",
    "\n",
    "    # Obtain the directory where the best model is saved.\n",
    "    print(\"You can use any of the following columns to get the best model: \\n{}.\".format(\n",
    "        [k for k in analysis.dataframe() if k.startswith(\"keras_info\")]))\n",
    "    print(\"=\" * 10)\n",
    "    logdir = analysis.get_best_logdir(\"keras_info/val_acc\", mode=\"max\")\n",
    "    print('Best model:',analysis.get_best_trial(metric='keras_info/val_acc', mode='max'), \n",
    "          'lr:', analysis.get_best_config(metric='keras_info/val_acc', mode='max')['lr'], 'unit:', analysis.get_best_config(metric='keras_info/val_acc', mode='max')['unit'], 'dropout:', analysis.get_best_config(metric='keras_info/val_acc', mode='max')['dropout'] )\n",
    "    # We saved the model as `model.h5` in the logdir of the trial.\n",
    "    from tensorflow.keras.models import load_model\n",
    "    tuned_model = load_model(logdir + \"/model.h5\", custom_objects =  {'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m})\n",
    "    tuned_model.summary()\n",
    "    image_size = 32\n",
    "    X_test = np.reshape(task_data[2],[-1, image_size, image_size])  \n",
    "    Y_test = task_data[3]\n",
    "    tuned_loss, tuned_accuracy, f1_score, precision, recall = tuned_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Loss is {:0.4f}\".format(tuned_loss))\n",
    "    print(\"Tuned accuracy is {:0.4f}\".format(tuned_accuracy))\n",
    "    print ('F1-score = {0}'.format(f1_score))\n",
    "    print ('Precision = {0}'.format(precision))\n",
    "    print ('Recall = {0}'.format(recall))\n",
    "    return(analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "\n",
    "#PBT population based sampling \n",
    "def mutation_pbtsearch(task_data, task_id=0, cpu_resouce_fed=4):\n",
    "    from ray.tune.schedulers import PopulationBasedTraining\n",
    "    from ray.tune.utils import validate_save_restore\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=20,\n",
    "        hyperparam_mutations={\n",
    "            # distribution for resampling\n",
    "            \"lr\": lambda: np.random.uniform(0.0001, 1),\n",
    "            # allow perturbations within this set of categorical values\n",
    "            \"unit\": [60, 80, 100], \"dropout\": [0.1, 0.2, 0.3], \n",
    "        }\n",
    "    )\n",
    "\n",
    "    old_dirs = os.listdir('/root/ray_results/')\n",
    "\n",
    "    ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "    ray.init(log_to_driver=False, num_cpus=cpu_resouce_fed)\n",
    "    ! rm -rf ~/ray_results/PBT_devnagri_task$task_id\n",
    "    analysis = tune.run(\n",
    "        tune_devnagri,\n",
    "        name=\"PBT_devnagri_task\"+str(task_id),\n",
    "        scheduler=scheduler,\n",
    "        reuse_actors=True,\n",
    "        verbose=1,\n",
    "        stop={\n",
    "            \"training_iteration\": 100,\n",
    "        },\n",
    "        num_samples=10,\n",
    "\n",
    "        # PBT starts by training many neural networks in parallel with random hyperparameters. \n",
    "        config={\n",
    "        \"lr\": tune.loguniform(0.001, 0.1),  \n",
    "        \"unit\": tune.uniform(50, 80),\n",
    "        \"dropout\": tune.loguniform(0.01, 0.3),\n",
    "    } )\n",
    "    time.sleep(1)\n",
    "    print(\"You can use any of the following columns to get the best model: \\n{}.\".format(\n",
    "        [k for k in analysis.dataframe() if k.startswith(\"keras_info\")]))\n",
    "    print(\"=\" * 10)\n",
    "    logdir = analysis.get_best_logdir(\"keras_info/val_acc\", mode=\"max\")\n",
    "    print('Best model:',analysis.get_best_trial(metric='keras_info/val_acc', mode='max'), \n",
    "          analysis.get_best_config(metric='keras_info/val_acc', mode='max'))\n",
    "    # We saved the model as `model.h5` in the logdir of the trial.\n",
    "    from tensorflow.keras.models import load_model\n",
    "    tuned_model = load_model(logdir + \"/model.h5\", custom_objects =  {'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m})\n",
    "    tuned_model.summary()\n",
    "    image_size = 32\n",
    "    X_test = np.reshape(task_data[2],[-1, image_size, image_size])\n",
    "    Y_test = task_data[3]\n",
    "    tuned_loss, tuned_accuracy, f1_score, precision, recall = tuned_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Loss is {:0.4f}\".format(tuned_loss))\n",
    "    print(\"Tuned accuracy is {:0.4f}\".format(tuned_accuracy))\n",
    "    print ('F1-score = {0}'.format(f1_score))\n",
    "    print ('Precision = {0}'.format(precision))\n",
    "    print ('Recall = {0}'.format(recall))\n",
    "    return(analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "\n",
    "#ASHA Schedular\n",
    "def ASHA_search(task_data, task_id=0, cpu_resouce_fed=4):\n",
    "    from ray.tune.schedulers import ASHAScheduler\n",
    "    ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "    ray.init(log_to_driver=False, num_cpus=cpu_resouce_fed)\n",
    "    custom_scheduler = ASHAScheduler(\n",
    "        metric='mean_accuracy',\n",
    "        mode=\"max\",\n",
    "        reduction_factor = 4,\n",
    "        grace_period=1)# TODO: Add a ASHA as custom scheduler here\n",
    "    hyperparameter_space={\n",
    "        \"lr\": tune.loguniform(0.001, 0.1),  \n",
    "        \"unit\": tune.uniform(50, 80),\n",
    "        \"dropout\": tune.loguniform(0.01, 0.3),\n",
    "    } \n",
    "    ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "    ray.init(log_to_driver=False, num_cpus=cpu_resouce_fed)\n",
    "    ! rm -rf ~/ray_results/ASHA_devnagri_task$task_id\n",
    "    analysis = tune.run(\n",
    "        tune_devnagri, \n",
    "        scheduler=custom_scheduler, \n",
    "        config=hyperparameter_space, \n",
    "        verbose=1,\n",
    "        num_samples=10,\n",
    "        #resources_per_trial={\"cpu\":4},\n",
    "        name=\"ASHA_devnagri_task\"+str(task_id)  # This is used to specify the logging directory.\n",
    "    )\n",
    "    time.sleep(1)\n",
    "    print(\"You can use any of the following columns to get the best model: \\n{}.\".format(\n",
    "        [k for k in analysis.dataframe() if k.startswith(\"keras_info\")]))\n",
    "    print(\"=\" * 10)\n",
    "    logdir = analysis.get_best_logdir(\"keras_info/acc\", mode=\"max\")\n",
    "    print('Best model:',analysis.get_best_trial(metric='keras_info/acc', mode='max'), \n",
    "          analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "    # We saved the model as `model.h5` in the logdir of the trial.\n",
    "    from tensorflow.keras.models import load_model\n",
    "    tuned_model = load_model(logdir + \"/model.h5\", custom_objects =  {'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m})\n",
    "    tuned_model.summary()\n",
    "    image_size = 32\n",
    "    X_test = np.reshape(task_data[2],[-1, image_size, image_size])\n",
    "    Y_test = task_data[3]\n",
    "    tuned_loss, tuned_accuracy, f1_score, precision, recall = tuned_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Loss is {:0.4f}\".format(tuned_loss))\n",
    "    print(\"Tuned accuracy is {:0.4f}\".format(tuned_accuracy))\n",
    "    print ('F1-score = {0}'.format(f1_score))\n",
    "    print ('Precision = {0}'.format(precision))\n",
    "    print ('Recall = {0}'.format(recall))\n",
    "    return(analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "\n",
    "\n",
    "#HyperOpt Search \n",
    "def hyperopt_search(task_data, task_id=0, cpu_resouce_fed=4):\n",
    "    from ray.tune.suggest import ConcurrencyLimiter\n",
    "    from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "    from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "    \n",
    "    search_space={\n",
    "        \"lr\": tune.loguniform(0.001, 0.1),  \n",
    "        \"unit\": tune.uniform(50, 80),\n",
    "        \"dropout\": tune.loguniform(0.01, 0.3),\n",
    "    }\n",
    "    current_best_params = [{\n",
    "    'lr': 0.01,\n",
    "    'unit': 25,\n",
    "    'dropout': 0.2,\n",
    "    }]\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "    \n",
    "    algo = HyperOptSearch(points_to_evaluate=current_best_params)\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=cpu_resouce_fed)\n",
    "    ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "    ray.init(log_to_driver=False, num_cpus=cpu_resouce_fed)\n",
    "    ! rm -rf ~/ray_results/BayesOptSearch_devnagri_task$task_id\n",
    "    analysis =tune.run(tune_Drebin,\n",
    "        name=\"hyperopt_devnagri_task\"+str(task_id), verbose = 1,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=algo,\n",
    "        num_samples=10, \n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        config=search_space,\n",
    "        stop={\"training_iteration\": 150})\n",
    "    time.sleep(1)\n",
    "    #from ray.tune import Analysis as analysis\n",
    "    #analysis = ray.tune.Analysis('/root/ray_results/BayesOptSearch_ASNM') \n",
    "    print(\"You can use any of the following columns to get the best model: \\n{}.\".format(\n",
    "        [k for k in analysis.dataframe() if k.startswith(\"keras_info\")]))\n",
    "    print(\"=\" * 10)\n",
    "    logdir = analysis.get_best_logdir(\"keras_info/acc\", mode=\"max\")\n",
    "    print('Best model:', analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "    # We saved the model as `model.h5` in the logdir of the trial.\n",
    "    from tensorflow.keras.models import load_model\n",
    "    tuned_model = load_model(logdir + \"/model.h5\", custom_objects =  {'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m})\n",
    "    tuned_model.summary()\n",
    "    image_size = 32\n",
    "    X_test = np.reshape(task_data[2],[-1, image_size, image_size])\n",
    "    Y_test = task_data[3]\n",
    "    tuned_loss, tuned_accuracy, f1_score, precision, recall = tuned_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Loss is {:0.4f}\".format(tuned_loss))\n",
    "    print(\"Tuned accuracy is {:0.4f}\".format(tuned_accuracy))\n",
    "    print ('F1-score = {0}'.format(f1_score))\n",
    "    print ('Precision = {0}'.format(precision))\n",
    "    print ('Recall = {0}'.format(recall))\n",
    "    return(analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "\n",
    "def BayesOptSearch(task_data, task_id=0, cpu_resouce_fed=4):\n",
    "    from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "    from ray.tune.suggest import ConcurrencyLimiter\n",
    "    from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "    \n",
    "    search_space={\n",
    "        \"lr\": tune.loguniform(0.001, 0.1),  \n",
    "        \"unit\": tune.uniform(50, 80),\n",
    "        \"dropout\": tune.loguniform(0.01, 0.3),\n",
    "    } \n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "    \n",
    "    algo = ConcurrencyLimiter(BayesOptSearch(utility_kwargs={\n",
    "        \"kind\": \"ucb\",\n",
    "        \"kappa\": 2.5,\n",
    "        \"xi\": 0.0\n",
    "        }, metric = 'mean_accuracy', mode = 'max'), \n",
    "        max_concurrent=cpu_resouce_fed)\n",
    "    ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "    ray.init(log_to_driver=False, num_cpus=cpu_resouce_fed)\n",
    "    ! rm -rf ~/ray_results/BayesOptSearch_devnagri_task$task_id\n",
    "    analysis =tune.run(tune_devnagri,\n",
    "        name=\"BayesOptSearch_devnagri_task\"+str(task_id), verbose = 1,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=algo,\n",
    "        num_samples=100, \n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        config=search_space,\n",
    "        stop={\"training_iteration\": 150})\n",
    "    time.sleep(1)\n",
    "    #from ray.tune import Analysis as analysis\n",
    "    #analysis = ray.tune.Analysis('/root/ray_results/BayesOptSearch_ASNM') \n",
    "    print(\"You can use any of the following columns to get the best model: \\n{}.\".format(\n",
    "        [k for k in analysis.dataframe() if k.startswith(\"keras_info\")]))\n",
    "    print(\"=\" * 10)\n",
    "    logdir = analysis.get_best_logdir(\"keras_info/acc\", mode=\"max\")\n",
    "    print('Best model:', analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "    # We saved the model as `model.h5` in the logdir of the trial.\n",
    "    from tensorflow.keras.models import load_model\n",
    "    tuned_model = load_model(logdir + \"/model.h5\", custom_objects =  {'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m})\n",
    "    tuned_model.summary()\n",
    "    image_size = 32\n",
    "    X_test = np.reshape(task_data[2],[-1, image_size, image_size])\n",
    "    Y_test = task_data[3]\n",
    "    tuned_loss, tuned_accuracy, f1_score, precision, recall = tuned_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Loss is {:0.4f}\".format(tuned_loss))\n",
    "    print(\"Tuned accuracy is {:0.4f}\".format(tuned_accuracy))\n",
    "    print ('F1-score = {0}'.format(f1_score))\n",
    "    print ('Precision = {0}'.format(precision))\n",
    "    print ('Recall = {0}'.format(recall))\n",
    "    return(analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "\n",
    "\n",
    "def NeverGradSearch(task_data, task_id=0, cpu_resouce_fed=4):\n",
    "    from ray.tune.suggest import ConcurrencyLimiter\n",
    "    from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "    from ray.tune.suggest.nevergrad import NevergradSearch\n",
    "    import nevergrad as ng\n",
    "    \n",
    "    search_space={\n",
    "        \"lr\": tune.loguniform(0.001, 0.1),  \n",
    "        \"unit\": tune.uniform(50, 80),\n",
    "        \"dropout\": tune.loguniform(0.01, 0.3),\n",
    "    } \n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "    \n",
    "    algo = NevergradSearch(\n",
    "        optimizer=ng.optimizers.OnePlusOne,\n",
    "        # space=space,  # If you want to set the space manually\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=cpu_resouce_fed)\n",
    "    ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "    ray.init(log_to_driver=False, num_cpus=cpu_resouce_fed)\n",
    "    ! rm -rf ~/ray_results/BayesOptSearch_devnagri_task$task_id\n",
    "    analysis =tune.run(tune_devnagri,\n",
    "        name=\"NeverGradSearch_devnagri_task\"+str(task_id), verbose = 1,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=algo,\n",
    "        num_samples=10, \n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        config=search_space,\n",
    "        stop={\"training_iteration\": 150})\n",
    "    time.sleep(1)\n",
    "    #from ray.tune import Analysis as analysis\n",
    "    #analysis = ray.tune.Analysis('/root/ray_results/BayesOptSearch_ASNM') \n",
    "    print(\"You can use any of the following columns to get the best model: \\n{}.\".format(\n",
    "        [k for k in analysis.dataframe() if k.startswith(\"keras_info\")]))\n",
    "    print(\"=\" * 10)\n",
    "    logdir = analysis.get_best_logdir(\"keras_info/acc\", mode=\"max\")\n",
    "    print('Best model:', analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "    # We saved the model as `model.h5` in the logdir of the trial.\n",
    "    from tensorflow.keras.models import load_model\n",
    "    tuned_model = load_model(logdir + \"/model.h5\", custom_objects =  {'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m})\n",
    "    tuned_model.summary()\n",
    "    image_size = 32\n",
    "    X_test = np.reshape(task_data[2],[-1, image_size, image_size])\n",
    "    Y_test = task_data[3]\n",
    "    tuned_loss, tuned_accuracy, f1_score, precision, recall = tuned_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Loss is {:0.4f}\".format(tuned_loss))\n",
    "    print(\"Tuned accuracy is {:0.4f}\".format(tuned_accuracy))\n",
    "    print ('F1-score = {0}'.format(f1_score))\n",
    "    print ('Precision = {0}'.format(precision))\n",
    "    print ('Recall = {0}'.format(recall))\n",
    "    return(analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "\n",
    "def OptunaSearch(task_data, task_id=0, cpu_resouce_fed=4):\n",
    "    from ray.tune.suggest import ConcurrencyLimiter\n",
    "    from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "    from ray.tune.suggest.optuna import OptunaSearch\n",
    "    \n",
    "    search_space={\n",
    "        \"lr\": tune.loguniform(0.001, 0.1),  \n",
    "        \"unit\": tune.uniform(50, 80),\n",
    "        \"dropout\": tune.loguniform(0.01, 0.3),\n",
    "    } \n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "    \n",
    "    algo = OptunaSearch(metric=\"mean_accuracy\",\n",
    "        mode=\"max\")\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=cpu_resouce_fed)\n",
    "    ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "    ray.init(log_to_driver=False, num_cpus=cpu_resouce_fed)\n",
    "    ! rm -rf ~/ray_results/BayesOptSearch_devnagri_task$task_id\n",
    "    analysis =tune.run(tune_devnagri,\n",
    "        name=\"OptunaSearch_devnagri_task\"+str(task_id), verbose = 1,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=algo,\n",
    "        num_samples=10, \n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        config=search_space,\n",
    "        stop={\"training_iteration\": 150})\n",
    "    time.sleep(1)\n",
    "    #from ray.tune import Analysis as analysis\n",
    "    #analysis = ray.tune.Analysis('/root/ray_results/BayesOptSearch_ASNM') \n",
    "    print(\"You can use any of the following columns to get the best model: \\n{}.\".format(\n",
    "        [k for k in analysis.dataframe() if k.startswith(\"keras_info\")]))\n",
    "    print(\"=\" * 10)\n",
    "    logdir = analysis.get_best_logdir(\"keras_info/acc\", mode=\"max\")\n",
    "    print('Best model:', analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "    # We saved the model as `model.h5` in the logdir of the trial.\n",
    "    from tensorflow.keras.models import load_model\n",
    "    tuned_model = load_model(logdir + \"/model.h5\", custom_objects =  {'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m})\n",
    "    tuned_model.summary()\n",
    "    image_size = 32\n",
    "    X_test = np.reshape(task_data[2],[-1, image_size, image_size])\n",
    "    Y_test = task_data[3]\n",
    "    tuned_loss, tuned_accuracy, f1_score, precision, recall = tuned_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Loss is {:0.4f}\".format(tuned_loss))\n",
    "    print(\"Tuned accuracy is {:0.4f}\".format(tuned_accuracy))\n",
    "    print ('F1-score = {0}'.format(f1_score))\n",
    "    print ('Precision = {0}'.format(precision))\n",
    "    print ('Recall = {0}'.format(recall))\n",
    "    return(analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "\n",
    "def ZOOptSearch(task_data, task_id=0, cpu_resouce_fed=4):\n",
    "    from ray.tune.suggest.zoopt import ZOOptSearch\n",
    "    from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "    from zoopt import ValueType  # noqa: F401\n",
    "    \n",
    "    search_space={\n",
    "        \"lr\": tune.loguniform(0.001, 0.1),  \n",
    "        \"unit\": tune.uniform(50, 80),\n",
    "        \"dropout\": tune.loguniform(0.01, 0.3),\n",
    "    }\n",
    "    scheduler = AsyncHyperBandScheduler()\n",
    "    num_samples = 10\n",
    "    zoopt_search_config = {\n",
    "        \"parallel_num\": cpu_resouce_fed,\n",
    "    }\n",
    "\n",
    "    algo = ZOOptSearch(\n",
    "        algo=\"Asracos\",  # only support ASRacos currently\n",
    "        budget=num_samples,\n",
    "        # dim_dict=space,  # If you want to set the space yourself\n",
    "        **zoopt_search_config)\n",
    "    ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "    ray.init(log_to_driver=False, num_cpus=cpu_resouce_fed)\n",
    "    ! rm -rf ~/ray_results/BayesOptSearch_devnagri_task$task_id\n",
    "    analysis =tune.run(tune_devnagri,\n",
    "        name=\"ZOOptSearch_devnagri_task\"+str(task_id), verbose = 1,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=algo,\n",
    "        num_samples=num_samples, \n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        config=search_space,\n",
    "        stop={\"training_iteration\": 150})\n",
    "    time.sleep(1)\n",
    "    #from ray.tune import Analysis as analysis\n",
    "    #analysis = ray.tune.Analysis('/root/ray_results/BayesOptSearch_ASNM') \n",
    "    print(\"You can use any of the following columns to get the best model: \\n{}.\".format(\n",
    "        [k for k in analysis.dataframe() if k.startswith(\"keras_info\")]))\n",
    "    print(\"=\" * 10)\n",
    "    logdir = analysis.get_best_logdir(\"keras_info/acc\", mode=\"max\")\n",
    "    print('Best model:', analysis.get_best_config(metric='keras_info/acc', mode='max'))\n",
    "    # We saved the model as `model.h5` in the logdir of the trial.\n",
    "    from tensorflow.keras.models import load_model\n",
    "    tuned_model = load_model(logdir + \"/model.h5\", custom_objects =  {'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m})\n",
    "    tuned_model.summary()\n",
    "    image_size = 32\n",
    "    X_test = np.reshape(task_data[2],[-1, image_size, image_size])\n",
    "    Y_test = task_data[3]\n",
    "    tuned_loss, tuned_accuracy, f1_score, precision, recall = tuned_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Loss is {:0.4f}\".format(tuned_loss))\n",
    "    print(\"Tuned accuracy is {:0.4f}\".format(tuned_accuracy))\n",
    "    print ('F1-score = {0}'.format(f1_score))\n",
    "    print ('Precision = {0}'.format(precision))\n",
    "    print ('Recall = {0}'.format(recall))\n",
    "    return(analysis.get_best_config(metric='keras_info/acc', mode='max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/6.25 GiB heap, 0.0/2.15 GiB objects<br>Result logdir: /root/ray_results/Random_devnagri_task22<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">   unit</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   val_loss</th><th style=\"text-align: right;\">    f1_m</th><th style=\"text-align: right;\">  precision_m</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_devnagri_7fd52_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0201999</td><td style=\"text-align: right;\">0.00277963</td><td style=\"text-align: right;\">76.122 </td><td style=\"text-align: right;\">0.99193 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         6.53774</td><td style=\"text-align: right;\">0.0204742  </td><td style=\"text-align: right;\">0.992317</td><td style=\"text-align: right;\">     0.992854</td></tr>\n",
       "<tr><td>tune_devnagri_7fd52_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0800977</td><td style=\"text-align: right;\">0.0687419 </td><td style=\"text-align: right;\">64.6523</td><td style=\"text-align: right;\">0.860968</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.64195</td><td style=\"text-align: right;\">0.323238   </td><td style=\"text-align: right;\">0.862184</td><td style=\"text-align: right;\">     0.863203</td></tr>\n",
       "<tr><td>tune_devnagri_7fd52_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0274417</td><td style=\"text-align: right;\">0.0340264 </td><td style=\"text-align: right;\">65.5525</td><td style=\"text-align: right;\">0.702128</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.89002</td><td style=\"text-align: right;\">0.56738    </td><td style=\"text-align: right;\">0.699841</td><td style=\"text-align: right;\">     0.704439</td></tr>\n",
       "<tr><td>tune_devnagri_7fd52_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.123244 </td><td style=\"text-align: right;\">0.00237379</td><td style=\"text-align: right;\">52.4222</td><td style=\"text-align: right;\">0.994497</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.62843</td><td style=\"text-align: right;\">0.0316694  </td><td style=\"text-align: right;\">0.994817</td><td style=\"text-align: right;\">     0.994993</td></tr>\n",
       "<tr><td>tune_devnagri_7fd52_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.199422 </td><td style=\"text-align: right;\">0.00763165</td><td style=\"text-align: right;\">54.7493</td><td style=\"text-align: right;\">0.988628</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.76614</td><td style=\"text-align: right;\">0.0294611  </td><td style=\"text-align: right;\">0.988928</td><td style=\"text-align: right;\">     0.988929</td></tr>\n",
       "<tr><td>tune_devnagri_7fd52_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0273745</td><td style=\"text-align: right;\">0.00353324</td><td style=\"text-align: right;\">62.4271</td><td style=\"text-align: right;\">0.994497</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.98916</td><td style=\"text-align: right;\">0.0112236  </td><td style=\"text-align: right;\">0.994643</td><td style=\"text-align: right;\">     0.994643</td></tr>\n",
       "<tr><td>tune_devnagri_7fd52_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0769428</td><td style=\"text-align: right;\">0.0180957 </td><td style=\"text-align: right;\">67.3951</td><td style=\"text-align: right;\">0.82832 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.99107</td><td style=\"text-align: right;\">0.425691   </td><td style=\"text-align: right;\">0.824885</td><td style=\"text-align: right;\">     0.830532</td></tr>\n",
       "<tr><td>tune_devnagri_7fd52_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0236908</td><td style=\"text-align: right;\">0.00340125</td><td style=\"text-align: right;\">58.5406</td><td style=\"text-align: right;\">0.993764</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         6.02574</td><td style=\"text-align: right;\">0.0161311  </td><td style=\"text-align: right;\">0.993928</td><td style=\"text-align: right;\">     0.993928</td></tr>\n",
       "<tr><td>tune_devnagri_7fd52_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0175643</td><td style=\"text-align: right;\">0.00451989</td><td style=\"text-align: right;\">54.3249</td><td style=\"text-align: right;\">0.918929</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.76334</td><td style=\"text-align: right;\">0.102492   </td><td style=\"text-align: right;\">0.911479</td><td style=\"text-align: right;\">     0.927104</td></tr>\n",
       "<tr><td>tune_devnagri_7fd52_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0189807</td><td style=\"text-align: right;\">0.0846956 </td><td style=\"text-align: right;\">78.8068</td><td style=\"text-align: right;\">0.508804</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         6.10505</td><td style=\"text-align: right;\">2.46659e+20</td><td style=\"text-align: right;\">0.510604</td><td style=\"text-align: right;\">     0.510604</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 07:09:26,646\tINFO tune.py:439 -- Total run time: 36.25 seconds (36.13 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use any of the following columns to get the best model: \n",
      "['keras_info/loss', 'keras_info/acc', 'keras_info/f1_m', 'keras_info/precision_m', 'keras_info/recall_m', 'keras_info/val_loss', 'keras_info/val_acc', 'keras_info/val_f1_m', 'keras_info/val_precision_m', 'keras_info/val_recall_m'].\n",
      "==========\n",
      "Best model: tune_devnagri_7fd52_00005 lr: 0.0035332382550846446 unit: 62.42705057243154 dropout: 0.027374485268163815\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "RNN (SimpleRNN)              (None, 62)                5890      \n",
      "_________________________________________________________________\n",
      "dense_output (Dense)         (None, 46)                2898      \n",
      "=================================================================\n",
      "Total params: 8,788\n",
      "Trainable params: 8,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Loss is 0.0134\n",
      "Tuned accuracy is 0.9945\n",
      "F1-score = 0.9943510293960571\n",
      "Precision = 0.9943510293960571\n",
      "Recall = 0.9943510293960571\n",
      "Train on 2726 samples, validate on 1274 samples\n",
      "Epoch 1/10\n",
      "2726/2726 [==============================] - 32s 12ms/sample - loss: 2.8859 - acc: 0.4215 - f1_m: 0.3286 - precision_m: 0.3950 - recall_m: 0.2955 - val_loss: 0.7484 - val_acc: 0.5808 - val_f1_m: 0.5031 - val_precision_m: 0.6052 - val_recall_m: 0.4309\n",
      "Epoch 2/10\n",
      "2726/2726 [==============================] - 29s 11ms/sample - loss: 0.6489 - acc: 0.6174 - f1_m: 0.6156 - precision_m: 0.6256 - recall_m: 0.6070 - val_loss: 0.5377 - val_acc: 0.7818 - val_f1_m: 0.7939 - val_precision_m: 0.8081 - val_recall_m: 0.7802\n",
      "Epoch 3/10\n",
      "2726/2726 [==============================] - 29s 11ms/sample - loss: 0.3466 - acc: 0.8632 - f1_m: 0.8656 - precision_m: 0.8722 - recall_m: 0.8591 - val_loss: 0.1275 - val_acc: 0.9772 - val_f1_m: 0.9654 - val_precision_m: 0.9767 - val_recall_m: 0.9545\n",
      "Epoch 4/10\n",
      "2726/2726 [==============================] - 29s 11ms/sample - loss: 0.0941 - acc: 0.9736 - f1_m: 0.9723 - precision_m: 0.9753 - recall_m: 0.9695 - val_loss: 0.0625 - val_acc: 0.9741 - val_f1_m: 0.9733 - val_precision_m: 0.9740 - val_recall_m: 0.9725\n",
      "Epoch 5/10\n",
      "2726/2726 [==============================] - 29s 11ms/sample - loss: 0.0457 - acc: 0.9842 - f1_m: 0.9838 - precision_m: 0.9851 - recall_m: 0.9825 - val_loss: 0.0271 - val_acc: 0.9937 - val_f1_m: 0.9930 - val_precision_m: 0.9937 - val_recall_m: 0.9922\n",
      "Epoch 6/10\n",
      "2726/2726 [==============================] - 29s 11ms/sample - loss: 0.0409 - acc: 0.9850 - f1_m: 0.9860 - precision_m: 0.9865 - recall_m: 0.9854 - val_loss: 0.0358 - val_acc: 0.9906 - val_f1_m: 0.9902 - val_precision_m: 0.9913 - val_recall_m: 0.9890\n",
      "Epoch 7/10\n",
      "2726/2726 [==============================] - 29s 11ms/sample - loss: 0.0623 - acc: 0.9839 - f1_m: 0.9833 - precision_m: 0.9845 - recall_m: 0.9821 - val_loss: 0.0446 - val_acc: 0.9914 - val_f1_m: 0.9906 - val_precision_m: 0.9914 - val_recall_m: 0.9898\n",
      "Epoch 8/10\n",
      "2726/2726 [==============================] - 29s 11ms/sample - loss: 0.0911 - acc: 0.9784 - f1_m: 0.9762 - precision_m: 0.9789 - recall_m: 0.9737 - val_loss: 0.0859 - val_acc: 0.9694 - val_f1_m: 0.9656 - val_precision_m: 0.9729 - val_recall_m: 0.9584\n",
      "Epoch 9/10\n",
      "2726/2726 [==============================] - 30s 11ms/sample - loss: 0.0441 - acc: 0.9864 - f1_m: 0.9845 - precision_m: 0.9870 - recall_m: 0.9821 - val_loss: 0.0195 - val_acc: 0.9922 - val_f1_m: 0.9922 - val_precision_m: 0.9922 - val_recall_m: 0.9922\n",
      "Epoch 10/10\n",
      "2726/2726 [==============================] - 30s 11ms/sample - loss: 0.0185 - acc: 0.9949 - f1_m: 0.9947 - precision_m: 0.9950 - recall_m: 0.9943 - val_loss: 0.0112 - val_acc: 0.9969 - val_f1_m: 0.9968 - val_precision_m: 0.9968 - val_recall_m: 0.9968\n",
      "Search algorithm random_search took 4306.695620536804.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pdb\n",
    "import time\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        last_model_stats = Model_Perf_save\n",
    "        for i,lay in enumerate(model.layers):\n",
    "            last_model_size = last_model_stats['shape'][-1][2*i+1][0]\n",
    "            layer_weights = lay.get_weights()\n",
    "            layer_weights[0][:last_model_stats['weights'][-1][3*i].shape[0],:last_model_size] = last_model_stats['weights'][-1][3*i]\n",
    "            if i != len(model.layers)-1: #for last (dense) layer\n",
    "                layer_weights[1][:last_model_stats['weights'][-1][3*i+1].shape[0],:last_model_size] = last_model_stats['weights'][-1][3*i+1]\n",
    "                layer_weights[2][:last_model_size] = last_model_stats['weights'][-1][3*i+2]\n",
    "            else:\n",
    "                layer_weights[1][:last_model_size] = last_model_stats['weights'][-1][3*i+1]\n",
    "            model.layers[i].set_weights(layer_weights)      \n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        last_model_stats = Model_Perf_save\n",
    "        for i,lay in enumerate(model.layers):\n",
    "            last_model_size = last_model_stats['shape'][-1][2*i+1][0]\n",
    "            layer_weights = lay.get_weights()\n",
    "            layer_weights[0][:last_model_stats['weights'][-1][3*i].shape[0],:last_model_size] = last_model_stats['weights'][-1][3*i]\n",
    "            if i != len(model.layers)-1: #for last (dense) layer\n",
    "                layer_weights[1][:last_model_stats['weights'][-1][3*i+1].shape[0],:last_model_size] = last_model_stats['weights'][-1][3*i+1]\n",
    "                layer_weights[2][:last_model_size] = last_model_stats['weights'][-1][3*i+2]\n",
    "            else:\n",
    "                layer_weights[1][:last_model_size] = last_model_stats['weights'][-1][3*i+1]\n",
    "            model.layers[i].set_weights(layer_weights)  \n",
    "        \n",
    "def create_task(data_path):\n",
    "    data = pickle.load(open(data_path, \"rb\"))\n",
    "    return data\n",
    "\n",
    "def measure_CPU_Mem():\n",
    "    import psutil\n",
    "    CPU_usage_dump = []\n",
    "    Mem_usage_dump = []  \n",
    "    while True:\n",
    "        CPU_usage_dump.append(psutil.cpu_percent())\n",
    "        Mem_usage_dump.append(psutil.virtual_memory().percent)\n",
    "        f_cpu=open(\"CPU_used.txt\", \"wb\")\n",
    "        f_mem=open(\"Mem_used.txt\", \"wb\")\n",
    "        pickle.dump(CPU_usage_dump, f_cpu) \n",
    "        pickle.dump(Mem_usage_dump, f_mem)     \n",
    "        f_cpu.close()\n",
    "        f_mem.close()\n",
    "        time.sleep(1)\n",
    "\n",
    "import multiprocessing\n",
    "task_list = create_task('devnagri_tasks.pkl')\n",
    "num_tasks=23\n",
    "num_labels=46\n",
    "Model_Perf_save = {}\n",
    "Model_Perf_save['tr_acc'] = []\n",
    "Model_Perf_save['val_acc'] = []\n",
    "Model_Perf_save['precision'] = []\n",
    "Model_Perf_save['recall'] = []\n",
    "Model_Perf_save['f1_score'] = []\n",
    "Model_Perf_save['shape'] = []\n",
    "Model_Perf_save['weights'] = []\n",
    "Model_Perf_save['learn_rate'] = []\n",
    "cpu_resouce_fed = 3\n",
    "for search_algo in [ \n",
    "                    random_search,\n",
    "                    #ASHA_search,\n",
    "                    #mutation_pbtsearch,\n",
    "                    #BayesOptSearch,\n",
    "                    #NeverGradSearch,\n",
    "                    #OptunaSearch,\n",
    "                    #ZOOptSearch,\n",
    "                    #hyperopt_search\n",
    "                    ]:\n",
    "    cpu_mem_collection = multiprocessing.Process(target=measure_CPU_Mem)\n",
    "    cpu_mem_collection.start()\n",
    "    start_time = time.time()\n",
    "    for task_id in range(0,num_tasks):\n",
    "        !rm -rf task_dataset.pkl\n",
    "        f = open('task_dataset.pkl', 'wb')\n",
    "        pickle.dump(task_list[task_id], f)\n",
    "        f.close()\n",
    "        hyper_param = search_algo(task_list[task_id], task_id, cpu_resouce_fed)\n",
    "        image_size = 32\n",
    "        if task_id == 0:\n",
    "            model = create_model(learning_rate=hyper_param['lr'], RNN_units=int(hyper_param['unit']), dropout=hyper_param['dropout'])\n",
    "            #call one of the search algorithm\n",
    "            history = model.fit(np.reshape(task_list[task_id][0],[-1, image_size, image_size]), \n",
    "                  task_list[task_id][1], batch_size=128, epochs=10, verbose=1,\n",
    "                  validation_data=(np.reshape(task_list[task_id][2],[-1, image_size, image_size]), task_list[task_id][3]))\n",
    "        else:\n",
    "            rnn_units = hyper_param[\"unit\"] + Model_Perf_save['shape'][-1][1][0]\n",
    "            model = create_model(learning_rate=hyper_param['lr'], RNN_units=int(rnn_units), dropout=hyper_param['dropout'])\n",
    "            history = model.fit(np.reshape(task_list[task_id][0],[-1, image_size, image_size]),\n",
    "                  task_list[task_id][1], batch_size=128, epochs=10, verbose=1,\n",
    "                  validation_data=(np.reshape(task_list[task_id][2],[-1, image_size, image_size]), task_list[task_id][3]), callbacks  = [LossHistory()])\n",
    "        loss_and_metrics = model.evaluate(np.reshape(task_list[task_id][0],[-1, image_size, image_size]), task_list[task_id][1], verbose=0)\n",
    "        Model_Perf_save['tr_acc'].append(loss_and_metrics[1])\n",
    "        loss_and_metrics = model.evaluate(np.reshape(task_list[task_id][4],[-1, image_size, image_size]), task_list[task_id][5], verbose=0)\n",
    "        Model_Perf_save['val_acc'].append(loss_and_metrics[1])\n",
    "        Model_Perf_save['f1_score'].append(loss_and_metrics[2])\n",
    "        Model_Perf_save['precision'].append(loss_and_metrics[3])\n",
    "        Model_Perf_save['recall'].append(loss_and_metrics[4])\n",
    "        Model_Perf_save['shape'].append([i.shape for i in model.get_weights()])\n",
    "        Model_Perf_save['learn_rate'].append(hyper_param[\"lr\"])\n",
    "        Model_Perf_save['weights'].append(model.get_weights()) \n",
    "    end_time = time.time()\n",
    "    print('Search algorithm {} took {}.'.format(search_algo.__name__, end_time - start_time))\n",
    "    \n",
    "    f=open(\"time_taken.txt\", \"a+\")\n",
    "    f.write('Time taken for algo {} is {}. \\n'.format(search_algo.__name__, end_time-start_time))\n",
    "\n",
    "    try:\n",
    "        f_cpu=open(\"CPU_used.txt\", \"rb\")\n",
    "        f_mem=open(\"Mem_used.txt\", \"rb\")\n",
    "        cpu_usage = pickle.load(f_cpu)\n",
    "        mem_usage = pickle.load(f_mem)\n",
    "        f_cpu.close()\n",
    "        f_mem.close()\n",
    "        cpu_mem_collection.terminate()\n",
    "        cpu_mem_collection.join()\n",
    "        f.write('CPU used is {}. \\n'.format(np.mean(cpu_usage)))\n",
    "        f.write('Memory used is {}. \\n \\n'.format(np.mean(mem_usage)))\n",
    "        f.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Algorithm: random_search\n",
      "Training accuracy for tasks: [0.9835821, 0.90614647, 0.9902366, 0.9672312, 0.9686462, 0.9791667, 0.9767184, 0.99036324, 0.9261385, 0.97915876, 0.9728097, 0.9898382, 0.90576494, 0.9966305, 0.9865017, 0.98083305, 0.9943375, 0.9757576, 0.9640045, 0.9674436, 0.98927516, 0.99435663, 0.9981658]\n",
      "Validation accuracy for tasks: [0.9757576, 0.89867496, 0.9813014, 0.9758567, 0.96043444, 0.98018295, 0.9698609, 0.9800307, 0.89689976, 0.9735489, 0.97337276, 0.98361874, 0.88408035, 0.9969902, 0.9789947, 0.975913, 0.9822354, 0.9698529, 0.96174043, 0.9537394, 0.9845679, 0.98584205, 0.99686027]\n",
      "Precision: [0.9739583, 0.90764815, 0.98119044, 0.9770358, 0.9617919, 0.9816828, 0.9731756, 0.97983646, 0.89956164, 0.9765785, 0.976411, 0.98363096, 0.88780665, 0.9970238, 0.9790946, 0.9763474, 0.98255813, 0.97020346, 0.96318173, 0.95531183, 0.98760486, 0.98655754, 0.99669474]\n",
      "Recall: [0.97321427, 0.89710367, 0.98119044, 0.972561, 0.96036583, 0.98018295, 0.9670078, 0.97907424, 0.89133525, 0.970289, 0.96438956, 0.98363096, 0.8816419, 0.9970238, 0.9754464, 0.9756098, 0.98255813, 0.97020346, 0.9568452, 0.9474982, 0.97789633, 0.98358136, 0.99669474]\n",
      "F1 score: [0.9735804, 0.902268, 0.98119044, 0.97475034, 0.96106756, 0.9809209, 0.9700243, 0.9794493, 0.8953537, 0.9733624, 0.97025794, 0.98363096, 0.8846646, 0.9970238, 0.9772294, 0.9759727, 0.98255813, 0.9702034, 0.95996314, 0.9513318, 0.9826114, 0.98504585, 0.99669474]\n",
      "Nodes in hidden layer:  [(32, 62), (32, 138), (32, 214), (32, 266), (32, 324), (32, 386), (32, 462), (32, 514), (32, 572), (32, 648), (32, 724), (32, 782), (32, 844), (32, 920), (32, 996), (32, 1054), (32, 1130), (32, 1206), (32, 1264), (32, 1340), (32, 1402), (32, 1460), (32, 1512)]\n",
      "Learning rates:  [0.0035332382550846446, 0.002779625851893939, 0.002779625851893939, 0.002373790881937378, 0.0034012474960852393, 0.0035332382550846446, 0.002779625851893939, 0.002373790881937378, 0.0034012474960852393, 0.002779625851893939, 0.002779625851893939, 0.0034012474960852393, 0.0035332382550846446, 0.002779625851893939, 0.002779625851893939, 0.0034012474960852393, 0.002779625851893939, 0.002779625851893939, 0.0034012474960852393, 0.002779625851893939, 0.0035332382550846446, 0.0034012474960852393, 0.002373790881937378]\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for j,search_algo in enumerate([\n",
    "                    #ASHA_search, \n",
    "                    random_search,\n",
    "                    #mutation_pbtsearch,\n",
    "                    #BayesOptSearch,\n",
    "                    #NeverGradSearch,\n",
    "                    #OptunaSearch,\n",
    "                    #ZOOptSearch,\n",
    "                    #hyperopt_search\n",
    "                    ]):\n",
    "    print('Search Algorithm: {0}'.format(search_algo.__name__))\n",
    "    print('Training accuracy for tasks:',Model_Perf_save['tr_acc'][j*num_tasks:j*num_tasks+num_tasks])\n",
    "    print('Validation accuracy for tasks:',Model_Perf_save['val_acc'][j*num_tasks:j*num_tasks+num_tasks])\n",
    "    print('Precision:',Model_Perf_save['precision'][j*num_tasks:j*num_tasks+num_tasks])\n",
    "    print('Recall:',Model_Perf_save['recall'][j*num_tasks:j*num_tasks+num_tasks])\n",
    "    print('F1 score:',Model_Perf_save['f1_score'][j*num_tasks:j*num_tasks+num_tasks])\n",
    "    print('Nodes in hidden layer: ',[(i[0])for i in Model_Perf_save['shape']][j*num_tasks:j*num_tasks+num_tasks])\n",
    "    print('Learning rates: ', Model_Perf_save['learn_rate'][j*num_tasks:j*num_tasks+num_tasks])\n",
    "    print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
